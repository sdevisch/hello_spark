# Performance 01: I/O, serialization, caching, partitions, joins, persistence

Generated: 2025-08-17 02:24 UTC

## Scope

Cross-cutting performance practices: IO formats, UDF vs native, caching, partitioning, broadcast, and data types.

## Console output

```text
ğŸš€ Starting Spark Performance Demo...
ğŸ“š Docs index: docs/index.md
ğŸ“ Temp directory: /var/folders/yv/qzkfbsw13_q1kpm5kdb1qpx40000gn/T/tmpci7_8ecv
ğŸŒ Spark UI: http://localhost:4040
============================================================
ğŸš€ SPARK PERFORMANCE DEMONSTRATION
============================================================
This demo will show common performance bottlenecks and solutions
Watch the Spark UI at http://localhost:4040 for detailed metrics!

============================================================
ğŸ—‚ï¸  DEMO 1: FILE FORMAT PERFORMANCE
============================================================

ğŸ“Š Generating 100,000 rows of sample data...

â±ï¸  Timing: SLOW - Writing CSV
   âœ… Completed in 1.29s

â±ï¸  Timing: SLOW - Reading CSV
   âœ… Completed in 0.89s (processed 99,989 rows)

â±ï¸  Timing: FAST - Writing Parquet
   âœ… Completed in 0.85s

â±ï¸  Timing: FAST - Reading Parquet
   âœ… Completed in 0.18s (processed 100,000 rows)

ğŸ“ˆ RESULTS:
   CSV Total Time:     2.18s
   Parquet Total Time: 1.03s
   Speedup: 2.1x faster

============================================================
ğŸ DEMO 2: SERIALIZATION ISSUES (Python UDFs)
============================================================

ğŸ“Š Generating 500,000 rows of sample data...

â±ï¸  Timing: SLOW - Python UDF with serialization
   âœ… Completed in 0.09s (processed 500,000 rows)

â±ï¸  Timing: FAST - Native Spark functions
   âœ… Completed in 0.06s (processed 500,000 rows)

ğŸ“ˆ RESULTS:
   Python UDF Time:    0.09s
   Native Spark Time:  0.06s
   Speedup: 1.7x faster
   ğŸ’¡ Tip: Use built-in Spark functions instead of Python UDFs!

============================================================
ğŸ’¾ DEMO 3: CACHING AND PERSISTENCE STRATEGIES
============================================================

ğŸ“Š Generating 800,000 rows of sample data...

ğŸŒ SLOW: No caching - recomputing transformations

â±ï¸  Timing: SLOW - First computation (no cache)
   âœ… Completed in 0.36s (processed 2 rows)

â±ï¸  Timing: SLOW - Second computation (no cache)
   âœ… Completed in 0.22s (processed 2 rows)

ğŸš€ FAST: With caching - compute once, reuse multiple times

â±ï¸  Timing: FAST - First computation (with cache)
   âœ… Completed in 0.49s (processed 2 rows)

â±ï¸  Timing: FAST - Second computation (from cache)
   âœ… Completed in 0.23s (processed 2 rows)

ğŸ“ˆ RESULTS:
   No Cache Total Time:   0.58s
   With Cache Total Time: 0.72s
   Speedup: 0.8x faster
   ğŸ’¡ Tip: Cache DataFrames that are used multiple times!

============================================================
ğŸ—‚ï¸  DEMO 4: PARTITIONING STRATEGIES
============================================================

ğŸ“Š Generating 1,000,000 rows of sample data...

ğŸŒ SLOW: Too many small partitions

â±ï¸  Timing: SLOW - Over-partitioned aggregation
   âœ… Completed in 2.10s (processed 2 rows)

ğŸš€ FAST: Optimal partitioning

â±ï¸  Timing: FAST - Optimally partitioned aggregation
   âœ… Completed in 0.50s (processed 2 rows)

ğŸ“ˆ RESULTS:
   Over-partitioned Time:  2.10s
   Optimal partition Time: 0.50s
   Speedup: 4.2x faster
   ğŸ’¡ Tip: Use 2-3 partitions per CPU core for optimal performance!

============================================================
ğŸ“¡ DEMO 5: BROADCAST VS SHUFFLE JOINS
============================================================

ğŸ“Š Generating 500,000 rows of sample data...
ğŸ“Š Large dataset: 500,000 rows
ğŸ“Š Small dataset: 2 rows

ğŸŒ SLOW: Regular join (shuffle)

â±ï¸  Timing: SLOW - Shuffle join
   âœ… Completed in 0.35s (processed 500,000 rows)

ğŸš€ FAST: Broadcast join

â±ï¸  Timing: FAST - Broadcast join
   âœ… Completed in 0.16s (processed 500,000 rows)

ğŸ“ˆ RESULTS:
   Shuffle Join Time:   0.35s
   Broadcast Join Time: 0.16s
   Speedup: 2.2x faster
   ğŸ’¡ Tip: Broadcast small tables (< 10MB) to avoid shuffles!

============================================================
ğŸ§  DEMO 6: MEMORY MANAGEMENT & PERSISTENCE
============================================================

ğŸ“Š Generating 300,000 rows of sample data...

ğŸ§ª Testing MEMORY_ONLY persistence:

â±ï¸  Timing: MEMORY_ONLY - Initial caching
   âœ… Completed in 0.36s

â±ï¸  Timing: MEMORY_ONLY - Access from cache
   âœ… Completed in 0.07s

ğŸ§ª Testing MEMORY_AND_DISK persistence:

â±ï¸  Timing: MEMORY_AND_DISK - Initial caching
   âœ… Completed in 0.21s

â±ï¸  Timing: MEMORY_AND_DISK - Access from cache
   âœ… Completed in 0.05s

ğŸ§ª Testing DISK_ONLY persistence:

â±ï¸  Timing: DISK_ONLY - Initial caching
   âœ… Completed in 0.18s

â±ï¸  Timing: DISK_ONLY - Access from cache
   âœ… Completed in 0.07s

ğŸ“ˆ PERSISTENCE STRATEGY COMPARISON:
Strategy             Cache Time   Access Time  Total     
------------------------------------------------------------
MEMORY_ONLY          0.36         0.07         0.43      
MEMORY_AND_DISK      0.21         0.05         0.26      
DISK_ONLY            0.18         0.07         0.25      

ğŸ’¡ Tips:
   - MEMORY_ONLY: Fastest but may cause OOM
   - MEMORY_AND_DISK: Good balance (recommended)
   - DISK_ONLY: Slowest but handles large datasets

============================================================
ğŸ‰ ALL DEMOS COMPLETED!
============================================================

ğŸ¯ KEY TAKEAWAYS:
1. ğŸ“ Use Parquet instead of CSV for better I/O performance
2. ğŸ Avoid Python UDFs - use native Spark functions
3. ğŸ’¾ Cache DataFrames that are accessed multiple times
4. ğŸ—‚ï¸  Use optimal partitioning (2-3 partitions per CPU core)
5. ğŸ“¡ Broadcast small tables to avoid expensive shuffles
6. ğŸ§  Choose appropriate persistence strategy for your use case

ğŸŒ Spark UI: http://localhost:4040
   Check the 'SQL' and 'Jobs' tabs to see execution plans!

â³ Skipping 60s keep-alive for docs generation

ğŸ§¹ Cleaning up...
   âœ… Removed temp directory: /var/folders/yv/qzkfbsw13_q1kpm5kdb1qpx40000gn/T/tmpci7_8ecv
   âœ… Stopped Spark session

ğŸ‘‹ Demo completed!
```
