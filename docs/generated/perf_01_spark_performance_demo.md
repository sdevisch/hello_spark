# Performance 01: I/O, serialization, caching, partitions, joins, persistence

Generated: 2025-08-10 16:29 UTC

## Console output

```text
ğŸš€ Starting Spark Performance Demo...
ğŸ“ Temp directory: /var/folders/yv/qzkfbsw13_q1kpm5kdb1qpx40000gn/T/tmptb1su2qz
ğŸŒ Spark UI: http://localhost:4040
============================================================
ğŸš€ SPARK PERFORMANCE DEMONSTRATION
============================================================
This demo will show common performance bottlenecks and solutions
Watch the Spark UI at http://localhost:4040 for detailed metrics!

============================================================
ğŸ—‚ï¸  DEMO 1: FILE FORMAT PERFORMANCE
============================================================

ğŸ“Š Generating 100,000 rows of sample data...

â±ï¸  Timing: SLOW - Writing CSV
   âœ… Completed in 1.37s

â±ï¸  Timing: SLOW - Reading CSV
   âœ… Completed in 1.03s (processed 99,989 rows)

â±ï¸  Timing: FAST - Writing Parquet
   âœ… Completed in 0.82s

â±ï¸  Timing: FAST - Reading Parquet
   âœ… Completed in 0.20s (processed 100,000 rows)

ğŸ“ˆ RESULTS:
   CSV Total Time:     2.40s
   Parquet Total Time: 1.02s
   Speedup: 2.3x faster

============================================================
ğŸ DEMO 2: SERIALIZATION ISSUES (Python UDFs)
============================================================

ğŸ“Š Generating 500,000 rows of sample data...

â±ï¸  Timing: SLOW - Python UDF with serialization
   âœ… Completed in 0.10s (processed 500,000 rows)

â±ï¸  Timing: FAST - Native Spark functions
   âœ… Completed in 0.05s (processed 500,000 rows)

ğŸ“ˆ RESULTS:
   Python UDF Time:    0.10s
   Native Spark Time:  0.05s
   Speedup: 1.8x faster
   ğŸ’¡ Tip: Use built-in Spark functions instead of Python UDFs!

============================================================
ğŸ’¾ DEMO 3: CACHING AND PERSISTENCE STRATEGIES
============================================================

ğŸ“Š Generating 800,000 rows of sample data...

ğŸŒ SLOW: No caching - recomputing transformations

â±ï¸  Timing: SLOW - First computation (no cache)
   âœ… Completed in 0.42s (processed 2 rows)

â±ï¸  Timing: SLOW - Second computation (no cache)
   âœ… Completed in 0.24s (processed 2 rows)

ğŸš€ FAST: With caching - compute once, reuse multiple times

â±ï¸  Timing: FAST - First computation (with cache)
   âœ… Completed in 0.37s (processed 2 rows)

â±ï¸  Timing: FAST - Second computation (from cache)
   âœ… Completed in 0.06s (processed 2 rows)

ğŸ“ˆ RESULTS:
   No Cache Total Time:   0.66s
   With Cache Total Time: 0.43s
   Speedup: 1.5x faster
   ğŸ’¡ Tip: Cache DataFrames that are used multiple times!

============================================================
ğŸ—‚ï¸  DEMO 4: PARTITIONING STRATEGIES
============================================================

ğŸ“Š Generating 1,000,000 rows of sample data...

ğŸŒ SLOW: Too many small partitions

â±ï¸  Timing: SLOW - Over-partitioned aggregation
   âœ… Completed in 2.21s (processed 2 rows)

ğŸš€ FAST: Optimal partitioning

â±ï¸  Timing: FAST - Optimally partitioned aggregation
   âœ… Completed in 0.33s (processed 2 rows)

ğŸ“ˆ RESULTS:
   Over-partitioned Time:  2.21s
   Optimal partition Time: 0.33s
   Speedup: 6.7x faster
   ğŸ’¡ Tip: Use 2-3 partitions per CPU core for optimal performance!

============================================================
ğŸ“¡ DEMO 5: BROADCAST VS SHUFFLE JOINS
============================================================

ğŸ“Š Generating 500,000 rows of sample data...
ğŸ“Š Large dataset: 500,000 rows
ğŸ“Š Small dataset: 2 rows

ğŸŒ SLOW: Regular join (shuffle)

â±ï¸  Timing: SLOW - Shuffle join
   âœ… Completed in 0.36s (processed 500,000 rows)

ğŸš€ FAST: Broadcast join

â±ï¸  Timing: FAST - Broadcast join
   âœ… Completed in 0.23s (processed 500,000 rows)

ğŸ“ˆ RESULTS:
   Shuffle Join Time:   0.36s
   Broadcast Join Time: 0.23s
   Speedup: 1.6x faster
   ğŸ’¡ Tip: Broadcast small tables (< 10MB) to avoid shuffles!

============================================================
ğŸ§  DEMO 6: MEMORY MANAGEMENT & PERSISTENCE
============================================================

ğŸ“Š Generating 300,000 rows of sample data...
âŒ Error during demo: type object 'StorageLevel' has no attribute 'MEMORY_ONLY_SER'

â³ Skipping 60s keep-alive for docs generation

ğŸ§¹ Cleaning up...
   âœ… Removed temp directory: /var/folders/yv/qzkfbsw13_q1kpm5kdb1qpx40000gn/T/tmptb1su2qz
   âœ… Stopped Spark session

ğŸ‘‹ Demo completed!
```
